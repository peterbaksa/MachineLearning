# Machine Learning project 
This project contains (and will be updated for) all machine learning techniques that I learned and practiced in 
python language. Most of them will be written as Jupyter Notebooks to better demonstrate how they work and there is
also more space to write readable and student friendly notes/pictures/videos..

### Current projects:
## [1. Linear Regression](#linear-regression)
## [2. Data preprocessing](#data-preprocessing)
## [3. Classification](#classification)
## [4. Neural Networks](#neural-networks)
## [5. Decision trees](#decision-trees)
## [6. Unsupervised Learning](#unsupervised-learning)
## [7. Reinforcement Learning](#reinforcement-learning)
## [7. Genetics algorithms](#genetics-algorithms)

----------------------------------

## Linear Regression
Check the sub-project project [Linear Regression](01_Linear_Regression/readme.md)

This project contains explanation of Linear Regression. Starting from base model to model with vectorisation, 
feature engineering, feature scaling and visualizations.

[1. Base of linear regression](01_Linear_Regression/01_base_of_linear_regression/readme.md)
- Contains model, cost function, gradient descent, training and visualization <br>

[2. Linear Regression on dataset](01_Linear_Regression/02_linear_regression_on_dataset/readme.md)
- Explains how to read data from CSV file, split to inputs and outputs and build model upon this data<br>

[3. Multiple feature](01_Linear_Regression/03_multiple_features/readme.md) 
- Explains how linear regression works with 2 features, compares 1 feature model vs 2 feature model

[4. Feature scaling](01_Linear_Regression/04_feature_scaling/readme.md)
- Explains what is data scaling and how to apply MinMax normalization on dataset

[5. Other feature scaling techiques](01_Linear_Regression/05_other_feature_scaling_techniques/readme.md)
- Explains other feature scaling methods and their implementation

[6. Vectorization](01_Linear_Regression/06_vectorization/readme.md)
- Explains how to appli vectorization on 1 feature model, 2 features model and multiple feature model + 
 computing speed compare: of non vectorized and vectorized model

[7. Feature engineering](01_Linear_Regression/07_feature_engineering/readme.md)
- Explains important of feature engineering - preprocessing work on input data - add new features and remove unnecessary once 

[8. Polynomial Logistic regression](01_Linear_Regression/08_Polynomial_logistic_regression/readme.md)
- Explains linear regression using polynomial function instead of straight line function as model of Linear Regression

[9. Regularization of Linear Regression](01_Linear_Regression/09_Regularization_of_linear_regression/readme.md)
- Explains base of regularization of Polynomial Linear Regression and importance of lambda parameter

## Data preprocessing

[1. Scaling and Normalization](02_Data_preprocessing/01_scaling_and_normalization/readme.md)
- Explains scaling and normalization methods of preprocessing data before work on ML model

[2. Encoding](02_Data_preprocessing/02_encoding/readme.md)
- Explains encoding techniques to change data types of model data to numbers - readable by model

## Classification
[1. Base of logistic regression](03_Classification/01_logistic_regression/01_base_of_logistic_regression/readme.md)
- Contains model, cost function, gradient descent, training and visualization <br>

[2. Polynomial Logistic regression](03_Classification/01_logistic_regression/02_polynomial_regression/readme.md)
- Explains Logistic regression using polynomial model function, instead of using straight line function

[3. Regularization of Logistic Regression](03_Classification/01_logistic_regression/03_regularization_od_logistic_regression/readme.md)
- Explains base of regularization of Polynomial Logistic Regression and importance of lambda parameter 

## Neural Networks

## Decision Trees

## Reinforcement Learning

## Unsupervised Learning

## Genetics algorithms